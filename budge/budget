#!/usr/bin/env python3
import argparse
from collections import namedtuple
import csv
import datetime
import hashlib
import json
from budget_defs import *
import os
import pdb
from re import sub
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.select import Select
from selenium.common.exceptions import NoSuchElementException
import sys
import time


report_file_name = '{}.txt'.format(
  datetime.datetime.today().strftime('%y-%b-%d_%H:%M_%a'))

report_file_path = os.path.join(REPORTS_DIR, report_file_name)

def report(str, ex=False):
   print(str)
   with open(report_file_path, 'a+') as f:
      f.write(str)
      f.write('\n')

class wait_for_page_load(object):

    MAX_WAIT = 20 # seconds
    
    def __init__(self, driver):
        self.driver = driver
    
    def __enter__(self):
       self.old_page = self.driver.find_element_by_tag_name('html')
    
    def page_has_loaded(self):
       new_page = self.driver.find_element_by_tag_name('html')
       return new_page.id != self.old_page.id
    
    def __exit__(self, *_):
       start_time = time.time() 
       while time.time() < start_time + self.MAX_WAIT:
           if self.page_has_loaded(): 
               return True 
           else: 
               time.sleep(0.1) 
       report('Timeout waiting for page load')
       raise Exception('Timeout waiting for page load')


def load_config():
    if not os.path.exists(CONFIG_PATH):
        report('Creating config "{}", edit this file and restart program'.format(CONFIG_PATH))
        with open(CONFIG_PATH, 'w+') as f:
            f.write(
            '{ "emoney_url": "https://wealth.emaplan.com/ema/SignIn",\n' + 
            '  "emoney_username": "a@b.com",\n' + 
            '  "emoney_pwd": "xxx",\n' + 
            '  "amazon_url": "https://www.amazon.com",\n' +
            '  "amazon_username_grd": "a@b.com",\n' +
            '  "amazon_password_grd": "xxx",\n' +
            '  "amazon_username_cfe": "a@b.com",\n' +
            '  "amazon_password_cfe": "xxx"\n' +

            '}\n')
    with open(CONFIG_PATH) as f:
        return json.load(f, object_hook=lambda d:
                      namedtuple('CONFIG', d.keys())(*d.values()))

def compute_budget(config, emoney_cache, amazon_cache, end_date, frequency):
    totals = {}

    # AMAZON_EXCEPTIONS_PATH has format { Order ID': 'Category'}
    emoney_exceptions = {}
    try:
        with open(EMONEY_EXCEPTIONS_PATH) as f:
            emoney_exceptions = json.load(f)
    except:
        report('Missing (or corrupt) EMONEY EXCEPTIONS, "{}", using empty file'.format(EMONEY_EXCEPTIONS_PATH))
    for date, expenses in emoney_cache.items():
        for expense in expenses:
            category, description = expense['Category'], expense['Description']
            if category == 'Amazon':
                continue # should appear in amazon_cache
            if description in['AMZN MKTP US', 'AMAZON.COM']:
                continue ; # These should have category as 'AMAZON', but just in case...
            if expense['Order ID'] in emoney_exceptions:
                category = emoney_exceptions[expense['Order ID']]
            if category not in BUDGET:
                report('Unknwon Emoney category: {}'.format(category))
                category = 'Unknown Emoney'
            if date >= end_date - datetime.timedelta(days=BUDGET[category]['span'] + frequency):
                amount = expense['Amount']
                if category == 'Kids':
                    if 'DERR' in description and amount > 120:
                       # < 120 is health, otherwise shared expense with GF
                       amount /= 2.0
                totals[category] = totals.get(category, 0.0) + amount

    # AMAZON_EXCEPTIONS_PATH has format { Order ID': 'Category'}
    try:
        with open(AMAZON_EXCEPTIONS_PATH) as f:
            amazon_exceptions = json.load(f)
    except:
        report('Missing (or corrupt) AMAZON EXCEPTIONS, "{}", using empty file'.format(AMAZON_EXCEPTIONS_PATH))
        amazon_exceptions = {}

    for date, expenses in amazon_cache.items():
        days_exceptions = amazon_exceptions.get(date, [])
        exceptions_map = { item['Order ID']: item['Category'] for item in days_exceptions }
        for expense in expenses:
            category = expense['Category']
            if category == 'date-marker':
                continue
            # translate amazon category, if possible
            if expense['Order ID'] in amazon_exceptions:
                category = amazon_exceptions[expense['Order ID']]
            #elif expense['Shipping Address City'] != 'Louisville': # assume shipping
            #    category = 'Gifts' # to other cities are gifts
            elif category in AMAZON_CATEGORIES: # try to 'translate' category
                category = AMAZON_CATEGORIES[category]
            elif category not in BUDGET:
                report('Unknown Amazon category: {}'.format(category))
                category = 'Unknown Amazon'
            if date >= end_date - datetime.timedelta(days=BUDGET[category]['span'] + frequency):
                amount = expense['Item Total']
                totals[category] = totals.get(category, 0.0) + amount
    total_budget = 0
    for key, item in BUDGET.items():
        total_budget += (item['limit'] / float(item['span'])) * YEAR
    report('Total annual budget: {}'.format(total_budget))
    reports = []
    # print sorted     
    for item in sorted(totals.keys()):
        limit = BUDGET[item]['limit']
        span = BUDGET[item]['span']
        total = totals[item]
        reports.append('{}: {} {:.2f}/{:.2f} {} days'.format(item.rjust(32, ' '), '***' if total > limit else '   ', total, limit, span))
    report('-----------------------------------')
    report('\n'.join(reports))


def scrape_emoney_spending(config, start_date, end_date):
    # all expenses >= start_date and <= end_date. If start_date == end_date,
    # you'll get 1 day.
    # We generate and scrape from the table rather than downloading .csv, as
    # the .csv route pops up a native save panel that we can't automate.
    expenses = {}
    start_date_range = start_date.strftime('%m/%d/%Y')
    end_date_range = end_date.strftime('%m/%d/%Y')
    driver = webdriver.Chrome('/usr/bin/chromedriver')
    try: 
        # Print how many days we'll fetch from emoney:
        report('Scraping emoney from {} to {}'.format(start_date_range, end_date_range))
        with wait_for_page_load(driver):
            driver.get(config.emoney_url)
            username = driver.find_element_by_id('Username')
            username.send_keys(config.emoney_username)
            pwd = driver.find_element_by_id('Password')
            pwd.send_keys(config.emoney_password)
            pwd.send_keys(Keys.RETURN)
        with wait_for_page_load(driver):
            spending = driver.find_element_by_partial_link_text('Spending')
            spending.click()
        time.sleep(3)
        with wait_for_page_load(driver):
            transactions = driver.find_element_by_partial_link_text('Transactions')
            transactions.click()
            time.sleep(3)
        # range_button 'Last 30 days'
        range_button = driver.find_element_by_xpath('//*[@id="Snb2Root"]/div/div/div[2]/div/div/div[1]/div[1]/div[1]/div/div[1]/div/span[1]')
        range_button.click()
        time.sleep(3)
        # 'Custom dates'
        custom_dates = driver.find_element_by_xpath('//*[@id="Snb2Root"]/div/div/div[2]/div/div/div[1]/div[1]/div[1]/div/div[1]/div[2]/div[4]')
        custom_dates.click()
        time.sleep(3)
        # set from_date, tab to to_date, and set it, then submit with \r\n:
        from_date = driver.find_element_by_xpath('//*[@id="spending-and-budgeting-filter-start-date-picker"]')
        from_date.send_keys('\b\b\b\b\b\b\b\b\b\b\b\b{}\t\t\b\b\b\b\b\b\b\b\b\b\b\b{}\r\n'.format(start_date_range, end_date_range))
        time.sleep(5)
        # Grab results from table
        table = driver.find_element_by_xpath('//*[@id="Snb2Root"]/div/div/div[2]/div/div/div[1]/div[3]/div/table/tbody')
        # The table lazy-loads, so we need to scroll it until all data is fetched
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            # Wait to load page
            time.sleep(3)
            # Calculate new scroll height and compare with last scroll height
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
               break
            last_height = new_height 
        # parse table into dict, keyed by date object, with values a list of all expenses
        # incurred on that date.

        for table_row in table.find_elements_by_xpath('./tr'):
            columns = [column.text for column in table_row.find_elements_by_xpath('./td')]
            if len(columns) == 5:  # skip entries without 5 columns...for example, divider row(s)
                expense_item = dict(zip(['Date','Description','Account','Category','Amount'], columns))
                # change 'Amount' value to a float
                expense_item['Amount'] = -float(sub(r'[^\d\-.]', '', expense_item['Amount']))
                expense_date  = datetime.datetime.strptime(expense_item.pop('Date')," %b %d, %Y").date()
                expenses_for_date = expenses.get(expense_date, [])
                expenses_for_date.append(expense_item)
                expenses[expense_date] = expenses_for_date
        # ensure start_date_range and end_date_range are in expenses
        if start_date not in expenses:
            expenses[start_date] = []
        if end_date not in expenses:
            expenses[end_date] = []
    except: # NoSuchElementException:
         report('No EMoney transactions for {} - {} (or scraping failure)'.format(start_date, end_date))
    driver.quit()
    return expenses

def create_amazon_csv(config, username, password, start_date, end_date):
    #  If date set to jan 15 2020 -> feb 15 2020, the generated
    #  filename will be 15-Jan-2020_to_16-Feb-2020.csv.
    #  Returns filepath of generated csv file, else returns None.
    expenses = {}
    driver = webdriver.Chrome('/usr/bin/chromedriver')
    # for windows, %-m/%#d/%Y  might work...
    start_month, start_day, start_year = start_date.strftime('%-m/%-d/%Y').split('/')
    end_month, end_day, end_year = end_date.strftime('%-m/%-d/%Y').split('/')
    downloads_dir = os.path.expanduser('~/Downloads')
    # remove older report(s)
    files = [file for file in os.listdir(downloads_dir)]
    files = [file for file in files if file.endswith('.csv') and '_to_' in file]
    [os.remove(os.path.join(downloads_dir, file)) for file in files]
    # Amazon generates a file name using range   startdate <= x < enddate
    csv_file_name = '{}_to_{}.csv'.format(
        start_date.strftime('%d-%b-%Y'),
        (end_date + datetime.timedelta(days=1)).strftime('%d-%b-%Y'))
    csv_file_path = os.path.join(downloads_dir, csv_file_name)

    with wait_for_page_load(driver):
        driver.get(config.amazon_url)
        time.sleep(2)
        sign_in_securely_button = driver.find_element_by_id('a-autoid-0-announce')
        sign_in_securely_button.click()
        time.sleep(1)
    with wait_for_page_load(driver):
        email_input = driver.find_element_by_id('ap_email')
        email_input.send_keys(username)
        continue_button = driver.find_element_by_id('continue')
        continue_button.click()
        time.sleep(1)
    with wait_for_page_load(driver):
        password_input = driver.find_element_by_id('ap_password')
        password_input.send_keys(password)
        sign_in_button = driver.find_element_by_id('signInSubmit')
        sign_in_button.click()
        time.sleep(1)
    with wait_for_page_load(driver):
        account_list_link = driver.find_element_by_id('nav-link-accountList')
        account_list_link.click()
        time.sleep(1)
    with wait_for_page_load(driver):
        download_order_reports_link = driver.find_element_by_link_text('Download order reports')
        download_order_reports_link.click()
        time.sleep(1)
    with wait_for_page_load(driver):
        report_month_start = Select(driver.find_element_by_id('report-month-start'))
        report_month_start.select_by_value('{}'.format(start_month))
        report_day_start = Select(driver.find_element_by_id('report-day-start'))
        report_day_start.select_by_value('{}'.format(start_day))  #'1' to '30' or whatever
        report_year_start = Select(driver.find_element_by_id('report-year-start'))
        report_year_start.select_by_value('{}'.format(start_year))  #'2020'  or whatever
        report_month_end = Select(driver.find_element_by_id('report-month-end'))
        report_month_end.select_by_value('{}'.format(end_month))  #'1' to '12'
        report_day_end = Select(driver.find_element_by_id('report-day-end'))
        report_day_end.select_by_value('{}'.format(end_day))  #'1' to '30' or whatever
        report_year_end = Select(driver.find_element_by_id('report-year-end'))
        report_year_end.select_by_value('{}'.format(end_year))  #'2020'  or whatever
        # generate new report
        report_confirm_button = driver.find_element_by_id('report-confirm')
        report_confirm_button.click()
    # Wait for file to be generated
    total_wait = 0
    while(total_wait < 60 * 5): # wait max 5 minutes
        time.sleep(1.0)
        total_wait += 1.0
        if os.path.exists(csv_file_path):
            driver.quit()
            # move file to ARCHIVE_DIR, prepend username-- to filename
            archive_file_path = os.path.join(ARCHIVE_DIR,
                '{}--{}'.format(username, csv_file_name))
            os.rename(csv_file_path, archive_file_path)
            return archive_file_path
    report('Failed to created amazon csv for {}'.format(archive_file_path, username))
    driver.quit()
    return None

def merge_amazon_csv(csv_file_path, username=None):
    if not(os.path.exists(csv_file_path)):
        report('Can\'t load csv file: {}'.format(csv_file_path))
        return False
    csv_file_name = os.path.basename(csv_file_path)
    if '--' in csv_file_name:
        username, csv_file_name = csv_file_name.split('--')
    else:
        archive_file_path = os.path.join(ARCHIVE_DIR,
            '{}--{}'.format(username, csv_file_name))
        os.rename(csv_file_path, archive_file_path)
        csv_file_path = archive_file_path
    # file format: dd-MMM-YYYY_to_dd-MMM-YYYY.csv
    from_date = datetime.datetime.strptime(csv_file_name[0:11], '%d-%b-%Y').date()
    to_date = datetime.datetime.strptime(csv_file_name[15:26], '%d-%b-%Y').date()    
    to_date = to_date - datetime.timedelta(days=1)
    expenses = {}
    # Load the amazon csv file, converting to our cache format ({ datetime.date: [ {expense_items}, {expense_items}....})
    with open(csv_file_path, 'r') as src:
        reader = csv.reader(src)
        columns = None
        for row in reader: # first row is column names
            if columns is None:
                columns = row
                continue
            if len(row) < 30:
                # If no data for time period, Amazon returns a single row with
                # 1 column containing 'No data found for this time period'. Normally, there
                # are 36 columns, and 'Item Total' (column 30) is the last we need.
                # Skip row if we don't have at least 30 columns.
                continue
            expense_item = dict(zip(columns, row))
            expense_item['Item Total'] = float(sub(r'[^\d\-.]', '', expense_item['Item Total'])) # want as float
            expense_date  = datetime.datetime.strptime(expense_item.pop('Order Date'),"%m/%d/%y").date() # mm/dd/yy
            expenses_for_date = expenses.get(expense_date, [])
            expenses_for_date.append(expense_item)
            expenses[expense_date] = expenses_for_date
    # Merge amazon csv items into amazon cache
    try:
        with open(AMAZON_CACHE_PATH) as f:
            cache = json.load(f)
    except:
        report('Missing (or corrupt) Amazon cache "{}", using empty cache'.format(AMAZON_CACHE_PATH))
        cache = {}
    for date in list(cache.keys()): # convert all keys to datetime.date objects
         cache[datetime.datetime.strptime(date, DATE_FORMAT).date()] = cache.pop(date)
    for date, expenses_for_date in expenses.items():
        cached_expenses_for_date = cache.get(date, [])
        cached_expenses_by_order_id = {expense['Order ID']:expense for expense in cached_expenses_for_date}
        expenses_by_order_id = {expense['Order ID']:expense for expense in expenses_for_date} 
        cached_expenses_by_order_id.update(expenses_by_order_id)
        cache[date] = list(cached_expenses_by_order_id.values())
    # Add date markers for start/end date, if no entries for user on that date
    user_key = 'Ordering Customer Email'
    marker_expense = {column:'' for column in columns if column is not 'Order Date'}
    marker_expense['Item Total'] = 0.0
    marker_expense['Category'] = 'date-marker'
    marker_expense[user_key] = username
    for date in (from_date, to_date):
        date_expenses = cache.get(date, [])
        date_users = {expense[user_key] for expense in date_expenses if user_key in expense}
        if username not in date_users:  # must add a date-marker
            marker_expense['Order ID'] = str(time.time()) # simply unique id
            date_expenses.append(marker_expense)
            cache[date] = date_expenses
    # Save the updated cache back to filesystem
    if os.path.exists(AMAZON_CACHE_PATH):
        os.rename(AMAZON_CACHE_PATH, AMAZON_CACHE_PATH + '.prev')
    with open(AMAZON_CACHE_PATH, 'w+') as f:
        f.write('{')
        items_knt = 0 ; items_limit = len(cache)
        for date, items in cache.items():
           f.write('\n\n"' + datetime.date.strftime(date, DATE_FORMAT) + '":\n')
           json.dump(items, f, indent=2)
           items_knt += 1
           if items_knt != items_limit:
               f.write(',')
        f.write('\n}\n')
    return cache


def update_amazon_cache(config, end_date, uncache):
    # Update the amazon cache, ensuring that it has entries for
    # the date range from end_date - (max_span in budget) to end_date.
    try:
        with open(AMAZON_CACHE_PATH) as f:
            cache = json.load(f)
    except:
        report('Missing (or corrupt) Amazon cache "{}", using empty cache'.format(AMAZON_CACHE_PATH))
        cache = {}
    dates = list(cache.keys())
    for date in dates: # convert all keys to datetime.date objects
         cache[datetime.datetime.strptime(date, DATE_FORMAT).date()] = cache.pop(date)
    # update the cache with more recent + older entries
    max_span = max([value['span'] for value in BUDGET.values()])
    start_date = end_date - datetime.timedelta(days=max_span)
    if not cache.keys(): # empty cache
        report('Initial amazon fetch cfe: {} {}'.format(start_date, end_date))
        filepath = create_amazon_csv(config, config.amazon_username_cfe, config.amazon_password_cfe, start_date, end_date)
        merge_amazon_csv(filepath) if filepath else report('Initial Amazon fetch for cfe failed.')
        report('Initial amazon fetch grd: {} {}'.format(start_date, end_date))
        filepath = create_amazon_csv(config, config.amazon_username_grd, config.amazon_password_grd, start_date, end_date)
        merge_amazon_csv(filepath) if filepath else report('Initial Amazon fetch for grd failed.')
    else:
        min_cfe = min_grd = datetime.date(year=2055,month=1,day=1)
        max_cfe = max_grd = datetime.date(year=1990,month=1,day=1)        
        for date, expenses in cache.items():
            for expense in expenses:
               if expense['Ordering Customer Email'] == config.amazon_username_cfe:
                   min_cfe = min(date, min_cfe)
                   max_cfe = max(date, max_cfe)
               elif expense['Ordering Customer Email'] == config.amazon_username_grd:                   
                   min_grd = min(date, min_grd)
                   max_grd = max(date, max_grd)
        max_cfe = max_cfe - datetime.timedelta(days=uncache)
        if end_date > max_cfe:
            from_date = max_cfe + datetime.timedelta(days=1)
            report('Fetching Amazon head for cfe: {} {}'.format(from_date, end_date))
            filepath = create_amazon_csv(config, config.amazon_username_cfe, config.amazon_password_cfe, from_date, end_date)
            merge_amazon_csv(filepath) if filepath else report('Amazon head fetch for cfe failed.')
        max_grd = max_grd - datetime.timedelta(days=uncache)
        if end_date > max_grd:
            from_date = max_grd + datetime.timedelta(days=1)
            report('Fetching Amazon head for grd: {} {}'.format(from_date, end_date))
            filepath = create_amazon_csv(config, config.amazon_username_grd, config.amazon_password_grd, max_grd, end_date)
            merge_amazon_csv(filepath) if filepath else report('Amazon head fetch for grd failed.')
        if start_date < min_cfe:
            to_date = min_cfe - datetime.timedelta(days=1)
            report('Fetching Amazon tail for cfe: {} {}'.format(start_date, to_date))
            filepath = create_amazon_csv(config, config.amazon_username_cfe, config.amazon_password_cfe, start_date, to_date)
            merge_amazon_csv(filepath) if filepath else report('Amazon tail fetch for cfe failed.')
        if start_date < min_grd:
            to_date = min_grd - datetime.timedelta(days=1)
            report('Fetching Amazon tail for grd: {} {}'.format(start_date, to_date))
            filepath = create_amazon_csv(config, config.amazon_username_grd, config.amazon_password_grd, start_date, to_date)
            merge_amazon_csv(filepath) if filepath else report('Amazon tail fetch for grd failed.')
    return cache
  
def update_emoney_cache(config, end_date, uncache):
    # load the cache file
    try:
        with open(EMONEY_CACHE_PATH) as f:
            cache = json.load(f)
    except exception as ex:
        report('Missing (or corrupt) Emoney cache "{}", using empty cache'.format(EMONEY_CACHE_PATH))
        cache = {}
    dates = list(cache.keys())
    for date in dates: # convert all keys to datetime.date objects
         cache[datetime.datetime.strptime(date, DATE_FORMAT).date()] = cache.pop(date)
    # update the cache with more recent + older entries
    max_span = max([value['span'] for value in BUDGET.values()])
    start_date = end_date - datetime.timedelta(days=max_span)
    if not cache.keys(): # empty cache
        report('Initial EMoney fetch: {} {}'.format(start_date, end_date))
        cache = scrape_emoney_spending(config, start_date, end_date)
    else:
        max_cache = max(cache.keys()) - datetime.timedelta(days=uncache)
        if end_date > max_cache:
            report('EMoney head fetch: {} {}'.format(max_cache + datetime.timedelta(days=1), end_date)) 
            cache.update(scrape_emoney_spending(config, max_cache + datetime.timedelta(days=1), end_date))
        min_cache = min(cache.keys())
        if start_date < min_cache:
            report('EMoney tail fetch: {} {}'.format(start_date, min_cache - datetime.timedelta(days=1)))
            cache.update(scrape_emoney_spending(config, start_date, min_cache - datetime.timedelta(days=1)))
    # Save the updated cache back to filesystem
    if os.path.exists(EMONEY_CACHE_PATH):
        os.rename(EMONEY_CACHE_PATH, EMONEY_CACHE_PATH + '.prev')

    def make_emoney_order_id(date, expense):
        # Identical expenses for a given date will have same hash, but
        # that's OK for our purposes, i.e. assigning Categories.
        hash_items = [str(value) for value in expense.values()] # all strings now...
        hash_items.sort()
        hash_key = ''.join(hash_items)
        hash_key += date
        return hashlib.md5(bytes(hash_key,'utf-8')).hexdigest()

    with open(EMONEY_CACHE_PATH, 'w+') as f:
        f.write('{')
        items_knt = 0 ; items_limit = len(cache)
        for date, items in cache.items():
           date_str = datetime.date.strftime(date, DATE_FORMAT)
           for item in items: # Create an (almost) unique Order ID's by hashing...
               if not 'Order ID' in item:
                   item['Order ID'] = make_emoney_order_id(date_str, item) 
           f.write('\n\n"' + date_str + '":\n')
           json.dump(items, f, indent=2)
           items_knt += 1
           if items_knt != items_limit:
               f.write(',')
        f.write('\n}\n')
    return cache


# main starts here:
def main():
    config = load_config()
    parser = argparse.ArgumentParser()
    parser.add_argument('-f', '--frequency', type=int, default=1)
    parser.add_argument('-u', '--uncache', type=int, default=21)
    parser.add_argument('-ma','--merge_amazon_csv',nargs=2,type=str)
    args = parser.parse_args()
    if args.merge_amazon_csv:
        file_path,username = args.merge_amazon_csv
        file_path = os.path.abspath(os.path.expanduser(file_path))
        merge_amazon_csv(file_path, username)
        return
    end_date = datetime.date.today() - datetime.timedelta(days=1)
    emoney_cache = update_emoney_cache(config, end_date, args.uncache)
    amazon_cache = update_amazon_cache(config, end_date, args.uncache)
    # ...analyse budget...
    compute_budget(config, emoney_cache, amazon_cache, end_date, args.frequency)
    return

try:
    main()
except BaseException as ex:
    import traceback
    traceback.print_exc()
    pdb.set_trace()
    
